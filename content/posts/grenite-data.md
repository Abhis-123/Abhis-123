---
title: "Part 1 Granite 3.0:  The Data Powering Granite 3.0"
description: "Data compression using Principal Component Analysis (PCA)"
dateString: Aug 2020
draft: false
tags: ["ML", "LLM", "data", "LLAMA3", "Instruction", "Finetuning"]
weight: 107
---

## Exploring the Data Powering Granite 3.0: A Look at Datasets and Their Impact

The quality and diversity of training data are crucial factors in the performance and capabilities of large language models (LLMs). The developers of the Granite 3.0 models recognized this and employed a multifaceted approach to data curation, encompassing a mix of curated web data, publicly available datasets, and, notably, a substantial portion of synthetically generated data. This blog post will explore the key insights gleaned from the sources regarding these datasets and their ultimate impact on the Granite 3.0 models.

### Embracing the Web and Beyond: Curated and Public Datasets

**The foundation of Granite 3.0's training data lies in a massive collection of text and code sourced from the web.** This curated web data includes content from academic, enterprise (financial, legal, biomedical), and code repositories. The sources emphasize the rigorous data preprocessing pipeline employed to ensure data quality, which includes text extraction, language identification, filtering for permissive licenses (especially for code), and adherence to IBM's URL blocking list.

**Beyond curated web data, the Granite 3.0 models also leverage several publicly available datasets.** These datasets, chosen for their permissive licenses and high quality, contribute to various domains, including:

*   **Code:** Github Code Clean, StarCoderdata, Code Contests
*   **Web:** FineWeb, DCLM-Baseline
*   **Multilingual:** MADLAD-12
*   **Instructions:** Code Instructions Alpaca, Glaive Function Calling V2, Self-OSS-Instruct-SC2

These diverse sources of real-world data helped equip the Granite 3.0 models with a broad base of knowledge and understanding. 

### The Rise of Synthetic Data: Addressing Data Scarcity and Targeting Capabilities

A key insight highlighted in the sources is the **increasing difficulty of relying solely on existing permissive datasets to train models with specialized capabilities**. To overcome this limitation and reduce reliance on costly human-annotated data, the developers of Granite 3.0 invested heavily in **synthetic data generation (SDG).**

The sources provide a detailed breakdown of how synthetic data was used to enhance various capabilities of the Granite 3.0 models:

*   **Generic Instruction Following:** Evol-Instruct and MagPie were employed to generate instruction-response pairs. Evol-Instruct iteratively evolves seed instructions into more complex versions, while MagPie leverages pre-query templates and LLMs to create diverse instructions.
*   **Coding:** Inspired by Starcoder2-Instruct, the developers expanded the OSS self-instruct pipeline to encompass multiple programming languages, using filtered pre-training data and a large code model to generate instruction-response pairs. They also created synthetic data for code-related tasks like code explanation, docstring generation, unit test generation, and code debugging. 
*   **Reasoning:** Two primary techniques were employed: code-assisted generation for algorithmic tasks and knowledge-based generation leveraging knowledge graphs like ATOMIC and Wikidata to create multi-hop reasoning data with chains of thought.
*   **Retrieval Augmented Generation (RAG):**  Synthetic data was generated by prompting LLMs with documents to produce user questions, then utilizing a retriever to select relevant passages and finally generating responses based on the retrieved information.
*   **Tool Use:** Synthetic data was created to address the complexities of tool calling, including multi-turn and nested calls, expanding beyond existing curated datasets like API-BLEND and APIGen.
*   **Cybersecurity:**  A two-step process was adopted. First, high-quality instructions were generated from predefined schemas based on security datasets. Then, the dataset's diversity and complexity were expanded using hybrid synthetic content-grounded data generation.
*   **Multilingual:**  The sources mention the use of synthetic data to improve machine translation quality, but specific details regarding techniques used for multilingual synthetic data are not provided.
*   **Safety:** Synthetic data was leveraged extensively for safety training and model alignment. Prompts and responses were generated across a range of scenarios, including direct and comparative questions, hypothetical situations, adversarial attacks, and multi-turn interactions, all designed to guide the model towards safe behavior.

**The sources emphasize the importance of rigorous quality filtering of synthetic data**. Techniques included removing short or unclear instructions, identifying duplicates, and using LLMs as judges to assess the category, quality, and difficulty of instructions.

### Data Mixtures and Hyperparameter Optimization: Fine-Tuning for Performance

**The sources highlight the careful consideration given to data mixtures and hyperparameter optimization during both pre-training and fine-tuning stages.**

*   **Pre-training Data Mixture:** A two-stage pre-training process was employed. Stage one focused on robustness across different domains using a data mixture search method inspired by distributionally robust language modeling. The objective was to minimize the weighted sum of relative domain losses with respect to a baseline.
*   **Post-training Data Mixture:** A combination of permissively licensed public data and internally collected synthetic data was used. The optimal data mixture was determined through experiments evaluating model improvement on internal benchmarks. The final mixture prioritized datasets that demonstrably enhanced performance in specific domains.

**The sources also discuss the use of a power scheduler and maximum update parameterization for hyperparameter search, which enabled efficient transfer of hyperparameters from small-scale experiments to large-scale training.**

### Evaluating the Impact: Benchmark Performance and Safety

The impact of the data curation and training methodologies is evident in the strong performance of the Granite 3.0 models across various benchmarks, including:

*   **General Knowledge and Instruction Following:** Outperforming similar models on tasks involving language understanding, reasoning, and code generation. 
*   **Function Calling:** Demonstrating superior tool use capabilities compared to other models.
*   **Cybersecurity:** Achieving high scores on security-related tasks, indicating the effectiveness of the curated and synthetic cybersecurity data.
*   **Retrieval Augmented Generation (RAG):** Exhibiting strong performance in tasks involving retrieving and utilizing relevant information from documents.
*   **Safety:**  Scoring well on safety benchmarks, demonstrating the effectiveness of the safety data and alignment techniques.

**The sources conclude that the Granite 3.0 models' success is a testament to the thoughtful integration of diverse data sources, particularly the strategic use of synthetic data generation to address data scarcity and target specific capabilities.** The emphasis on responsible AI is also evident in the focus on data governance, permissive licenses, and comprehensive safety evaluations. 
